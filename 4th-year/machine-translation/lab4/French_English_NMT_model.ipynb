{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUSdFfsWjE-p"
      },
      "source": [
        "We will build Fr-En MT system --\n",
        "Replace the paths wherever necessary --\n",
        "Use the dataset uploaded in the Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QUAY8y_Ro4j"
      },
      "outputs": [],
      "source": [
        "!pip3 install OpenNMT-py ## install OpenNMT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gg2ib1C7dBt3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/stephen/Desktop/machine-translation-labs/lab4\n"
          ]
        }
      ],
      "source": [
        "## check the current path and the contents\n",
        "## upload the dataset sharded\n",
        "!pwd   \n",
        "#!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B8tdgc3xs0hW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  en-fr.txt.zip\n",
            "  inflating: README                  \n",
            "  inflating: LICENSE                 \n",
            "  inflating: TED2020.en-fr.en        \n",
            "  inflating: TED2020.en-fr.fr        \n",
            "  inflating: TED2020.en-fr.xml       \n"
          ]
        }
      ],
      "source": [
        "# Unzip the data\n",
        "!unzip en-fr.txt.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktxQYkm1hfbM"
      },
      "outputs": [],
      "source": [
        "##split the dataset into train dev and test  dev and test both containing 1000 sentence each\n",
        "## use the train_test_split function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PzuogmZfBv0P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
            "  from cryptography.utils import int_from_bytes\n",
            "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
            "  from cryptography.utils import int_from_bytes\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: mosestokenizer in /home/stephen/.local/lib/python3.8/site-packages (1.2.1)\n",
            "Requirement already satisfied: docopt in /home/stephen/.local/lib/python3.8/site-packages (from mosestokenizer) (0.6.2)\n",
            "Requirement already satisfied: uctools in /home/stephen/.local/lib/python3.8/site-packages (from mosestokenizer) (1.3.0)\n",
            "Requirement already satisfied: openfile in /home/stephen/.local/lib/python3.8/site-packages (from mosestokenizer) (0.0.7)\n",
            "Requirement already satisfied: toolwrapper in /home/stephen/.local/lib/python3.8/site-packages (from mosestokenizer) (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install mosestokenizer ## tokenisation using Moses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsoPkdLYdib9"
      },
      "outputs": [],
      "source": [
        "from mosestokenizer import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC4wQOdgeUrD"
      },
      "outputs": [],
      "source": [
        "## Below we are performing tokenization for training set, perfrom it for validation and test set as well\n",
        "## remember to replace the paths wherever applicable\n",
        "\n",
        "from openfile import openfile\n",
        "inputfile = openfile(\"train file name src\")\n",
        "outputfile = openfile(\"train file name tgt\", \"w\")\n",
        "tokenize = MosesTokenizer('en')\n",
        "with inputfile,outputfile:\n",
        "  for line in inputfile:\n",
        "    print(*tokenize(line),file=outputfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZkLeTGUiIim"
      },
      "outputs": [],
      "source": [
        "### Vocabulary configuration file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZR1E7D9s8MV"
      },
      "outputs": [],
      "source": [
        "config = '''# fr_en_build_vocab.yaml\n",
        "## Where the samples will be written\n",
        "save_data: path\n",
        "## Where the vocab(s) will be written\n",
        "src_vocab: path\n",
        "tgt_vocab: path\n",
        "\n",
        "# Corpus opts:\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src:  train path\n",
        "        path_tgt:  train path\n",
        "    valid:\n",
        "        path_src: valid path\n",
        "        path_tgt:  valid path\n",
        "\n",
        "src_seq_length: 150\n",
        "tgt_seq_length: 150\n",
        "\n",
        "# silently ignore empty lines in the data\n",
        "skip_empty_level: silent\n",
        "'''\n",
        "with open(\"fr_en_build_vocab.yaml\", \"w\") as config_yaml: # w+ means append use \"w\"\n",
        "  config_yaml.write(config)\n",
        "\n",
        "!cat fr_en_build_vocab.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I6EefNITSZ7"
      },
      "outputs": [],
      "source": [
        "config = '''# fr_en_training.yaml\n",
        "\n",
        "## Where the samples will be written\n",
        "# save_data: path\n",
        "## Where the vocab(s) will be written\n",
        "# src_vocab: src-vocab path\n",
        "# tgt_vocab: tgt-vocab path\n",
        "# Prevent overwriting existing files in the folder\n",
        "overwrite: False\n",
        "\n",
        "# Corpus opts:\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: train file path source\n",
        "        path_tgt: train file target\n",
        "    valid:\n",
        "        path_src: valid-src\n",
        "        path_tgt: valid tgt\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Remove or modify these lines for bigger files\n",
        "train_steps: 300000\n",
        "valid_steps: 5000\n",
        "learning_rate: 2\n",
        "\n",
        "early_stopping: 5\n",
        "\n",
        "src_vocab: path \n",
        "tgt_vocab: path\n",
        "\n",
        "# Train on a single GPU\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Where to save the checkpoints\n",
        "save_model: path/model\n",
        "save_checkpoint_steps: 5000\n",
        "# train_steps: 300000\n",
        "# valid_steps: 5000\n",
        "'''\n",
        "with open(\"en_fr_training.yaml\", \"w\") as config_yaml:\n",
        "  config_yaml.write(config)\n",
        "\n",
        "!cat fr_en_training.yaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCgY-Kk0xQLs"
      },
      "outputs": [],
      "source": [
        "!onmt_build_vocab -config fr_en_build_vocab.yaml -n_sample -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3mAf9l7Rj_8j"
      },
      "outputs": [],
      "source": [
        "!onmt_train -config fr_en_training.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svq-a81pkJ8k"
      },
      "outputs": [],
      "source": [
        "!onmt_translate -model model_step_10000.pt -src test file -output opfile -gpu 0 -verbose ## replace model_step_10000.pt with your best checpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dla1jX8Riu9Q"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq5U5EVO6wLN"
      },
      "outputs": [],
      "source": [
        "### detokenise the output file and compute the Bleu score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x8FRslAiv5y"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "import sacrebleu\n",
        "from sacremoses import MosesDetokenizer\n",
        "md = MosesDetokenizer(lang='lang')\n",
        "\n",
        "refs = []\n",
        "\n",
        "with open(\"path/test.fr\") as test:\n",
        "    for line in test: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        refs.append(line)\n",
        "    \n",
        "#print(\"Reference 1st sentence:\", refs[0])\n",
        "\n",
        "refs = [refs]  # Yes, it is a list of list(s) as required by sacreBLEU\n",
        "\n",
        "\n",
        "# Open the translation file by the NMT model and detokenize the predictions\n",
        "preds = []\n",
        "\n",
        "with open(\"output file\") as pred:  \n",
        "    for line in pred: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        preds.append(line)  \n",
        "\n",
        "\n",
        "# Calculate and print the BLEU score\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "print(bleu.score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "French-English_NMT-model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
